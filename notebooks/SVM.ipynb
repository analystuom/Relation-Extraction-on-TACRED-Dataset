{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9736443-f3bc-4c28-9ae4-ffa9c3fdc7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\melod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\melod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\melod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab') # Download the 'punkt_tab' resource\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\" Load JSON data from a file. \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\" Preprocess text by lowercasing, removing numbers/punctuation, and filtering stopwords. \"\"\"\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    words = word_tokenize(text)  # Tokenize words\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return \" \".join(words)\n",
    "\n",
    "def extract_features_and_labels(data):\n",
    "    \"\"\" Extract features (text, POS, dependency relations, entity types) and labels. \"\"\"\n",
    "    texts, pos_tags, dep_rels, entity_types, labels = [], [], [], [], []\n",
    "    \n",
    "    for item in data:\n",
    "        sentence = \" \".join(item[\"token\"])\n",
    "        texts.append(preprocess_text(sentence))\n",
    "        pos_tags.append(\" \".join(item[\"stanford_pos\"]))\n",
    "        dep_rels.append(\" \".join(item[\"stanford_deprel\"]))\n",
    "        entity_types.append(item[\"subj_type\"] + \" \" + item[\"obj_type\"])\n",
    "        labels.append(item[\"relation\"])\n",
    "    \n",
    "    return texts, pos_tags, dep_rels, entity_types, labels\n",
    "\n",
    "# Load dataset\n",
    "train_data = load_data(\"train.json\")\n",
    "test_data = load_data(\"test.json\")\n",
    "\n",
    "# Extract features and labels\n",
    "X_train_text, X_train_pos, X_train_dep, X_train_ent, y_train = extract_features_and_labels(train_data)\n",
    "X_test_text, X_test_pos, X_test_dep, X_test_ent, y_test = extract_features_and_labels(test_data)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer_text = TfidfVectorizer()\n",
    "vectorizer_pos = TfidfVectorizer()\n",
    "vectorizer_dep = TfidfVectorizer()\n",
    "vectorizer_ent = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer_text.fit_transform(X_train_text)\n",
    "X_test_tfidf = vectorizer_text.transform(X_test_text)\n",
    "X_train_pos_tfidf = vectorizer_pos.fit_transform(X_train_pos)\n",
    "X_test_pos_tfidf = vectorizer_pos.transform(X_test_pos)\n",
    "X_train_dep_tfidf = vectorizer_dep.fit_transform(X_train_dep)\n",
    "X_test_dep_tfidf = vectorizer_dep.transform(X_test_dep)\n",
    "X_train_ent_tfidf = vectorizer_ent.fit_transform(X_train_ent)\n",
    "X_test_ent_tfidf = vectorizer_ent.transform(X_test_ent)\n",
    "\n",
    "# Combine all feature vectors\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_pos_tfidf, X_train_dep_tfidf, X_train_ent_tfidf])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_pos_tfidf, X_test_dep_tfidf, X_test_ent_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorizers\n",
    "with open('svm_vectorizer_text.pickle', 'wb') as file:\n",
    "    pickle.dump(vectorizer_text, file)\n",
    "\n",
    "with open('svm_vectorizer_pos.pickle', 'wb') as file:\n",
    "    pickle.dump(vectorizer_pos, file)\n",
    "\n",
    "with open('svm_vectorizer_dep.pickle', 'wb') as file:\n",
    "    pickle.dump(vectorizer_dep, file)\n",
    "\n",
    "with open('svm_vectorizer_ent.pickle', 'wb') as file:\n",
    "    pickle.dump(vectorizer_ent, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4982dc11-9a8a-4f05-9135-51cc11c0f64c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train distribution: Counter({'no_relation': 55112, 'relation': 13012})\n",
      "Balanced train distribution: Counter({'relation': 55112, 'no_relation': 55112})\n",
      "Original test distribution: Counter({'no_relation': 12184, 'relation': 3325})\n",
      "Balanced test distribution: Counter({'no_relation': 3325, 'relation': 3325})\n",
      "Binary Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " no_relation       0.57      0.94      0.71      3325\n",
      "    relation       0.84      0.30      0.44      3325\n",
      "\n",
      "    accuracy                           0.62      6650\n",
      "   macro avg       0.71      0.62      0.58      6650\n",
      "weighted avg       0.71      0.62      0.58      6650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to binary (relation vs. no_relation)\n",
    "binary_labels_train = np.array([\"relation\" if lbl != \"no_relation\" else \"no_relation\" for lbl in y_train])\n",
    "binary_labels_test = np.array([\"relation\" if lbl != \"no_relation\" else \"no_relation\" for lbl in y_test])\n",
    "\n",
    "# Balance training data using SMOTE\n",
    "print(\"Original train distribution:\", Counter(binary_labels_train))\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_combined, binary_labels_train)\n",
    "print(\"Balanced train distribution:\", Counter(y_train_balanced))\n",
    "\n",
    "# Balance test data using RandomUnderSampler\n",
    "print(\"Original test distribution:\", Counter(binary_labels_test))\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_test_balanced, y_test_balanced = rus.fit_resample(X_test_combined, binary_labels_test)\n",
    "print(\"Balanced test distribution:\", Counter(y_test_balanced))\n",
    "\n",
    "# Train binary SVM classifier\n",
    "svm_binary = SVC(kernel='rbf', class_weight='balanced', C=1.0, probability=True)\n",
    "svm_binary.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_binary = svm_binary.predict(X_test_balanced)\n",
    "print(\"Binary Classification Report:\")\n",
    "print(classification_report(y_test_balanced, y_pred_binary))\n",
    "\n",
    "# Save model\n",
    "with open('svm_binary_model.pickle', 'wb') as file:\n",
    "    pickle.dump(svm_binary, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4a9c67-054c-4083-8939-6e63b024d1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Classification Report:\n",
      "                                     precision    recall  f1-score   support\n",
      "\n",
      "                org:alternate_names       0.86      0.79      0.82       213\n",
      "           org:city_of_headquarters       0.99      0.94      0.96        82\n",
      "        org:country_of_headquarters       0.91      0.97      0.94       108\n",
      "                      org:dissolved       0.50      0.50      0.50         2\n",
      "                        org:founded       0.97      0.97      0.97        37\n",
      "                     org:founded_by       0.55      0.09      0.15        68\n",
      "                      org:member_of       0.12      0.17      0.14        18\n",
      "                        org:members       0.00      0.00      0.00        31\n",
      "    org:number_of_employees/members       1.00      1.00      1.00        19\n",
      "                        org:parents       0.35      0.37      0.36        62\n",
      "org:political/religious_affiliation       1.00      1.00      1.00        10\n",
      "                   org:shareholders       0.00      0.00      0.00        13\n",
      "org:stateorprovince_of_headquarters       1.00      0.96      0.98        51\n",
      "                   org:subsidiaries       0.21      0.36      0.27        44\n",
      "          org:top_members/employees       0.69      0.73      0.71       346\n",
      "                        org:website       1.00      1.00      1.00        26\n",
      "                            per:age       1.00      1.00      1.00       200\n",
      "                per:alternate_names       0.14      0.55      0.23        11\n",
      "                 per:cause_of_death       1.00      1.00      1.00        52\n",
      "                        per:charges       1.00      1.00      1.00       103\n",
      "                       per:children       0.36      0.35      0.36        37\n",
      "            per:cities_of_residence       0.84      0.95      0.89       189\n",
      "                  per:city_of_birth       0.33      1.00      0.50         5\n",
      "                  per:city_of_death       0.50      0.07      0.12        28\n",
      "         per:countries_of_residence       0.78      0.91      0.84       148\n",
      "               per:country_of_birth       0.00      0.00      0.00         5\n",
      "               per:country_of_death       0.00      0.00      0.00         9\n",
      "                  per:date_of_birth       0.62      0.56      0.59         9\n",
      "                  per:date_of_death       0.93      0.94      0.94        54\n",
      "                    per:employee_of       0.58      0.69      0.63       264\n",
      "                         per:origin       0.88      0.81      0.85       132\n",
      "                   per:other_family       0.37      0.62      0.46        60\n",
      "                        per:parents       0.56      0.40      0.47        88\n",
      "                       per:religion       1.00      1.00      1.00        47\n",
      "               per:schools_attended       1.00      0.57      0.72        30\n",
      "                       per:siblings       0.95      0.35      0.51        55\n",
      "                         per:spouse       0.79      0.68      0.73        66\n",
      "       per:stateorprovince_of_birth       0.43      0.38      0.40         8\n",
      "       per:stateorprovince_of_death       0.00      0.00      0.00        14\n",
      "  per:stateorprovinces_of_residence       0.80      0.83      0.81        81\n",
      "                          per:title       1.00      1.00      1.00       500\n",
      "\n",
      "                           accuracy                           0.78      3325\n",
      "                          macro avg       0.63      0.62      0.61      3325\n",
      "                       weighted avg       0.79      0.78      0.77      3325\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melod\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\melod\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\melod\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Extract only \"relation\" instances for multi-class classification\n",
    "relation_indices = [i for i, label in enumerate(y_train) if label != \"no_relation\"]\n",
    "X_train_relation = X_train_combined[relation_indices]\n",
    "y_train_relation = np.array(y_train)[relation_indices]\n",
    "\n",
    "# Train multi-class SVM WITHOUT SMOTE (no oversampling)\n",
    "svm_multi = SVC(kernel='rbf', class_weight='balanced', C=1.0, probability=True)\n",
    "svm_multi.fit(X_train_relation, y_train_relation)\n",
    "\n",
    "# Extract only \"relation\" instances from test data\n",
    "relation_indices_test = [i for i, label in enumerate(y_test) if label != \"no_relation\"]\n",
    "X_test_relation = X_test_combined[relation_indices_test]\n",
    "y_test_relation = np.array(y_test)[relation_indices_test]\n",
    "\n",
    "# Predict multi-class labels\n",
    "y_pred_multi = svm_multi.predict(X_test_relation)\n",
    "print(\"Multi-class Classification Report:\")\n",
    "print(classification_report(y_test_relation, y_pred_multi))\n",
    "\n",
    "# Save model\n",
    "with open('svm_multi_model.pickle', 'wb') as file:\n",
    "    pickle.dump(svm_multi, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f7caaff-07ea-4b77-b75c-41abeb727000",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Relation: per:countries_of_residence\n"
     ]
    }
   ],
   "source": [
    "def predict_relation(user_text, entity1, entity2):\n",
    "    \"\"\" Predict the relation between two entities in a user-provided sentence. \"\"\"\n",
    "    processed_input = preprocess_text(user_text)\n",
    "    input_vector_text = vectorizer_text.transform([processed_input])\n",
    "    input_vector_pos = vectorizer_pos.transform([\"unknown\"])\n",
    "    input_vector_dep = vectorizer_dep.transform([\"unknown\"])\n",
    "    input_vector_ent = vectorizer_ent.transform([\"unknown unknown\"])\n",
    "    \n",
    "    input_vector_combined = np.hstack([input_vector_text.toarray(), input_vector_pos.toarray(), input_vector_dep.toarray(), input_vector_ent.toarray()])\n",
    "    binary_prediction = svm_binary.predict(input_vector_combined)[0]\n",
    "    \n",
    "    if binary_prediction == \"no_relation\":\n",
    "        return \"No relation detected\"\n",
    "    \n",
    "    multi_prediction = svm_multi.predict(input_vector_combined)[0]\n",
    "    return multi_prediction\n",
    "\n",
    "# Example prediction\n",
    "user_sentence = \"Barack Obama was born in Hawaii.\"\n",
    "entity1 = \"Barack Obama\"\n",
    "entity2 = \"Hawaii\"\n",
    "predicted_relation = predict_relation(user_sentence, entity1, entity2)\n",
    "print(\"Predicted Relation:\", predicted_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8aa031e-61fd-4d28-afbf-ac78f869f133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input a sentence: David was born in Italy.\n",
      "Please input the first entity: David\n",
      "Please input the second entity: Italy\n",
      "Predicted Relation: per:countries_of_residence\n"
     ]
    }
   ],
   "source": [
    "# Example User Input\n",
    "user_sentence = input(\"Please input a sentence:\")\n",
    "entity1 = input(\"Please input the first entity:\")\n",
    "entity2 = input(\"Please input the second entity:\")\n",
    "predicted_relation = predict_relation(user_sentence, entity1, entity2)\n",
    "print(\"Predicted Relation:\", predicted_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb445014-4126-4249-aaad-cb285904bce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85101a9-d92a-4c24-9c46-bbad6172b72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9dd91a-9fc4-49ca-8562-140d874bc4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4eedf9-2887-4a1d-bee1-24c192785326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
